{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brabeem/anaconda3/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "from network import net\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from torch.distributions import MultivariateNormal\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "    def __init__(self,env):\n",
    "        ##environment initialized##\n",
    "        self.env = env\n",
    "        \n",
    "        ##number of dim in state and number of possible action are initilized##\n",
    "        self.obs_dim = self.env.observation_space.shape[0]\n",
    "        self.act_dim = self.env.action_space.shape[0]\n",
    "        \n",
    "        ##actor and critic network##\n",
    "        self.actor = net(self.obs_dim,self.act_dim)\n",
    "        self.critic = net(self.obs_dim,1)\n",
    "\n",
    "        ##initialize some of the hyperparameters here##\n",
    "        self._init_hyperparameters()\n",
    "\n",
    "        ## create a covariance matrix ##\n",
    "        cov_var = torch.full((self.act_dim,),0.5)\n",
    "        self.cov_mat = torch.diag(cov_var)\n",
    "\n",
    "        ##define optimizers##\n",
    "        self.optim_actor = optim.Adam(self.actor.parameters(),lr=self.lr)\n",
    "        self.optim_critic = optim.Adam(self.critic.parameters(),lr=self.lr)\n",
    "    ##function to optimize the nets##\n",
    "    def learn(self):\n",
    "        t = 0\n",
    "        actor_loss_window = deque([],maxlen=100)\n",
    "        critic_loss_window = deque([],maxlen=100)\n",
    "        actor_loss_full = []\n",
    "        critic_loss_full = [] \n",
    "        while t < self.total_timestep:\n",
    "            ##collect a batch##\n",
    "            batch_obs,batch_act,batch_rtgs,batch_logprobs,batch_lens = self.rollout()\n",
    "            #print(\"Rolled OUT: \",t)\n",
    "            t += torch.sum(batch_lens) ##increment the timesteps##\n",
    "            for _ in range(self.num_of_iterations_per_batch):\n",
    "                ##update each iteration from the same batch here##\n",
    "                V,current_logprobs = self.evaluate(batch_obs,batch_act)\n",
    "                ratio = torch.exp(current_logprobs - batch_logprobs)\n",
    "                #print(\"batchReturnsShape: \",batch_rtgs.shape)\n",
    "                #print(\"ValueShape: \",V.shape)\n",
    "                Ad = batch_rtgs - V\n",
    "                ##actor loss##\n",
    "                #print(\"Calculating Actor loss........\")\n",
    "                surr1 = Ad * ratio\n",
    "                surr2 = torch.clamp(ratio,1-self.clip,1+self.clip) * Ad\n",
    "                actor_loss = (-torch.min(surr1,surr2).sum()/(batch_lens.shape[0]))\n",
    "                actor_loss_full.append(actor_loss)\n",
    "                actor_loss_window.append(actor_loss)\n",
    "                ##critic loss##\n",
    "                #print(\"calculating Critic loss.........\")\n",
    "                critic_loss = nn.MSELoss()(V,batch_rtgs)\n",
    "                critic_loss_full.append(critic_loss)\n",
    "                critic_loss_window.append(critic_loss)\n",
    "                ##update actor##\n",
    "                self.optim_actor.zero_grad()\n",
    "                actor_loss.backward(retain_graph=True)\n",
    "                self.optim_actor.step()\n",
    "                ##update critic##\n",
    "                self.optim_critic.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.optim_critic.step()\n",
    "            sys.stdout.flush()\n",
    "            print(\"\\r{}/{}\".format(t,self.total_timestep),\"Average Actor Loss:\",torch.mean(torch.tensor(actor_loss_window)),\"Averge critic Loss:\",torch.mean(torch.tensor(critic_loss_window)),end=\"\")\n",
    "        plt.subplot(211)\n",
    "        plt.plot(np.log(np.arange(len(actor_loss_full))),actor_loss_full)\n",
    "        plt.subplot(212)\n",
    "        plt.plot(np.log(np.arange(len(critic_loss_full))),critic_loss_full)\n",
    "        torch.save(self.actor.state_dict(),\"actorModel\")\n",
    "    ##function to initialize all hyperparameters##\n",
    "    def _init_hyperparameters(self):\n",
    "        self.total_timestep = 500000\n",
    "        self.num_of_iterations_per_batch = 5\n",
    "        self.clip = 0.2\n",
    "        self.time_step_per_batch = 1000\n",
    "        self.max_timestep_per_eps = 200\n",
    "        self.gamma = 0.99\n",
    "        self.lr = 0.005\n",
    "    ##function to extract a batch of timesteps without keeping track of gradients##\n",
    "    def rollout(self):\n",
    "        batch_obs = []\n",
    "        batch_act = []\n",
    "        batch_logprobs = []\n",
    "        batch_lens = []\n",
    "        t_step = 0\n",
    "        batch_rews = []\n",
    "        while t_step < self.time_step_per_batch:\n",
    "            state = self.env.reset()\n",
    "            eps_rews = []\n",
    "            for n in range(self.max_timestep_per_eps):\n",
    "                t_step += 1\n",
    "                batch_obs.append(state)\n",
    "                act,logprobs = self.get_action(state) ## define this function ##\n",
    "                state,rew,done,_=self.env.step(act)\n",
    "                eps_rews.append(rew)\n",
    "                batch_act.append(act)\n",
    "                batch_logprobs.append(logprobs)\n",
    "                if done:\n",
    "                    break\n",
    "            batch_lens.append(n + 1)\n",
    "            batch_rews.append(eps_rews)\n",
    "        #print(\"Collected a batch!\")\n",
    "        batch_rtgs = self.ret_rtgs(batch_rews)## define this function ##\n",
    "        batch_obs = torch.tensor(batch_obs,dtype=torch.float)\n",
    "        batch_act = torch.tensor(batch_act,dtype=torch.float)\n",
    "        batch_logprobs = torch.tensor(batch_logprobs,dtype=torch.float)\n",
    "        batch_lens = torch.tensor(batch_lens,dtype=torch.float)\n",
    "        return batch_obs,batch_act,batch_rtgs,batch_logprobs,batch_lens\n",
    "    ##function to get the action and its log_prob given a state and current policy##\n",
    "    def get_action(self,state):\n",
    "        mean = self.actor(state)\n",
    "        dist = MultivariateNormal(mean,self.cov_mat)\n",
    "        action = dist.sample()\n",
    "        logprob = dist.log_prob(action)\n",
    "        return action.detach().numpy(),logprob.detach()\n",
    "    ##function to return sum of rewards given a list of step rewards##\n",
    "    def ret_rtgs(self,batch_rews):\n",
    "        batch_rtgs = []\n",
    "        for ep in reversed(batch_rews):\n",
    "            discounted_reward = 0\n",
    "            for rew in reversed(ep):\n",
    "                discounted_reward = rew + discounted_reward * self.gamma\n",
    "                batch_rtgs.insert(0,discounted_reward)\n",
    "        return torch.tensor(batch_rtgs,dtype=torch.float)\n",
    "    ##function to return the log_probs and value given state and action taken##\n",
    "    def evaluate(self,batch_obs,batch_act):\n",
    "       mean = self.actor(batch_obs)\n",
    "       dist = MultivariateNormal(mean,self.cov_mat)\n",
    "       log_prob = dist.log_prob(batch_act)\n",
    "\n",
    "       V = self.critic(batch_obs).squeeze()\n",
    "       return V,log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PPO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6ca4b6a3cc3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BipedalWalker-v3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlearning_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlearning_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PPO' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\")\n",
    "learning_agent = PPO(env)\n",
    "learning_agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brabeem/anaconda3/lib/python3.7/site-packages/gym/envs/box2d/bipedal_walker.py:564: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\")\n",
    "state = env.reset()\n",
    "new_agent = PPO(env)\n",
    "new_agent.actor.load_state_dict(torch.load(\"actorModel\"))\n",
    "for i in range(2000):\n",
    "    env.render(state)\n",
    "    mean = new_agent.actor(state)\n",
    "    dist = MultivariateNormal(mean,new_agent.cov_mat)\n",
    "    sam = dist.sample().detach()\n",
    "    log_prob = dist.log_prob(sam).detach()\n",
    "    state,_,_,_=env.step(sam)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a16280308017aeff994d6f82fa17551f539861d5dfe26be943aae8003a5aefe3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
